{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###           Classification of Cars using Feed Forward Network(Perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required Libraries\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dataset\\\\class_1',\n",
       " 'Dataset\\\\class_2',\n",
       " 'Dataset\\\\class_3',\n",
       " 'Dataset\\\\class_4']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the file from Different Directories\n",
    "directories = [name for name in glob.glob('Dataset/class_*') if os.path.isdir(name)]\n",
    "directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class_1', 'class_2', 'class_3', 'class_4']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preparing the final target class used in the Algorithm\n",
    "classes = []\n",
    "for directory in directories:\n",
    "    classes.append(directory.split('\\\\')[1])\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The final feature dataset should be in the form of nested tuples,dictionaries,lists\n",
    "#The Format specified is\n",
    "#[(class,{actual_pathname:'absolute path to png file'})]\n",
    "\n",
    "feature_data = []                      #The data used for training purpose\n",
    "for directory in directories:\n",
    "    paths = glob.glob(directory+'/*.jpg')\n",
    "    current_class = directory.split('\\\\')[1]\n",
    "    for path in paths:\n",
    "        feature_dictionary = {}        #A dummy Dictionary used for converting it to tuples \n",
    "        feature_dictionary['path'] = path\n",
    "        feature_data.append((current_class,feature_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We Extract data from Different Directories and then To feed it to the Neural Network Shuffle it \n",
    "#so than the Network is not biased and manual mappings cannot be done\n",
    "\n",
    "random.shuffle(feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Data for fast access\n",
    "with open('feature_data.pkl','wb') as f:\n",
    "    pickle.dump(feature_data,f)\n",
    "with open('classes.pkl','wb') as f:\n",
    "    pickle.dump(classes,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Designing the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrievng the Stored Data from the DB\n",
    "with open('feature_data.pkl','rb') as f:\n",
    "    feature_data = pickle.load(f)\n",
    "with open('classes.pkl','rb') as f:\n",
    "    classes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Algorithm Begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 Initialise Weights and Biases to small random values and set the Learning Rate\n",
    "\n",
    "# Constants\n",
    "bias = 1                        # Dummy Feature for use in setting constant factor in Training.\n",
    "Train_Test_Ratio = 0.6           # Default Ratio of data to be used in Training vs. Testing.\n",
    "Iterations = 20                 # Number of Training Iterations.(Epochs)\n",
    "lr = 0.1                       # Set learning rate(Alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Differernt Activation Functions Here Choice of Activation Functions?\n",
    "def sigmoid(x):\n",
    "    return (1/1+np.exp(-x))\n",
    "\n",
    "def ReLu(x):\n",
    "    return max(0,x)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the Perceptron Class\n",
    "\n",
    "class MultiClassPerceptron:\n",
    "    \n",
    "    accuracy = 0\n",
    "    \n",
    "# A MultiClass Perceptron Model for training of Data,Predicting Analysing Features etc\n",
    "\n",
    "    def __init__(self,classes,feature_data,hidden,train_test_ratio=Train_Test_Ratio,iterations=Iterations):\n",
    "        self.classes = classes\n",
    "        self.hidden = hidden\n",
    "        self.feature_data = feature_data\n",
    "        self.ratio = train_test_ratio\n",
    "        self.iterations = iterations\n",
    "        \n",
    "        #Splitting Feature Data into Training and Testing Set For The Second step i;e training pair s:t\n",
    "        random.shuffle(self.feature_data)\n",
    "        self.train_set = self.feature_data[:int(len(self.feature_data)*self.ratio)]\n",
    "        self.test_set = self.feature_data[int(len(self.feature_data)*self.ratio):]\n",
    "        \n",
    "        #Initialising the Weight Vectors in the Form of a Dictionary with BIAS terms between Input and Hidden Layers\n",
    "        self.weight_vectors = {c:np.array([0.0 for _ in range(4901)]) for c in self.hidden}\n",
    "        \n",
    "        #Initialising the Weight Vectors in the Form of a Dictionary with BIAS terms between Hidden and Output Layers\n",
    "        \n",
    "        self.weight_hidd_out = {p:np.array([0.0 for _ in range(len(self.hidden))]) for p in self.classes}\n",
    "        #Done with Step1 now train the network\n",
    "        \n",
    "    def train(self):\n",
    "\n",
    "                \n",
    "        \"\"\"Train the Multi-Class Perceptron algorithm using the following method:\n",
    "\n",
    "        Step2 : For each training pair s:t, the data (formatted as a feature vector) is read in, and the dot\n",
    "                product is taken with each unique weight vector (which are all initially set to 0). \n",
    "                \n",
    "        Step3 : Calculate the net I/P Yin = bias + summation(x*wts).\n",
    "        \n",
    "        Step4 : Pass it to the Activations Functions.\n",
    "        \n",
    "        Step5 : The class that\n",
    "        yields the highest product is the class to which the data belongs. In the case this class is the\n",
    "        correct value (matches with the actual category to which the data belongs), nothing happens, and the\n",
    "        next data point is read in. However, in the case that the predicted value is wrong, the weight vectors a\n",
    "        re corrected as follows: The feature vector is subtracted from the predicted weight vector, and added to\n",
    "        the actual (correct) weight vector. This makes sense, as we want to reject the wrong answer, and accept\n",
    "        the correct one.\"\"\"\n",
    "        #For Number of Epochs\n",
    "        for _ in range(self.iterations):\n",
    "        \n",
    "            for category,feature_dictionary in self.train_set:\n",
    "\n",
    "                #Obtaininig the Image and preprocessing it\n",
    "                image = cv2.imread(feature_dictionary['path'])\n",
    "                #Dimension Required for Resizing the Data\n",
    "                dim = (70,70)\n",
    "\n",
    "                resized_image = cv2.resize(image,dim,cv2.INTER_AREA)\n",
    "\n",
    "                input_array = []\n",
    "\n",
    "                #Normalising the Image between 0 and 1\n",
    "\n",
    "                input_array = [np.mean(resized_image[i][j]/255) for i in range(resized_image.shape[0])\n",
    "                              for j in range(resized_image.shape[1])]\n",
    "                #Appending Extra Bias for each Input\n",
    "                input_array.append(bias)\n",
    "                input_vector = np.array(input_array)\n",
    "                \n",
    "                #Initialise ARG_MAX we perform our activations with that\n",
    "                arg_max,predicted_class = 0,self.classes[0]\n",
    "                \n",
    "                #This is the Input for Hidden Layers\n",
    "                input_array_hidden = []\n",
    "                \n",
    "                #We Make MultiClass Decision Rule Based on the above conditions\n",
    "                for index in self.hidden:\n",
    "                    current_activation = np.dot(input_vector,self.weight_vectors[index])\n",
    "                    current_activation = ReLu(current_activation)\n",
    "                    \n",
    "                    input_array_hidden.append(current_activation)\n",
    "                    #if current_activation >= arg_max:\n",
    "                        #arg_max,predicted_class = current_activation,index\n",
    "                \n",
    "                for index in self.classes:\n",
    "                    current_activation = np.dot(input_array_hidden,self.weight_hidd_out[index])\n",
    "                    if current_activation >= arg_max:\n",
    "                        arg_max,predicted_class = current_activation,index\n",
    "                        \n",
    "                #Updation of Weights And Biases\n",
    "                #that is y!=t Update weights and biases\n",
    "                #If y!=t we reduce its probability\n",
    "                if not(category == predicted_class):\n",
    "                    for index in self.hidden:\n",
    "                        target = int(category[-1])\n",
    "                        self.weight_vectors[int(index)] += [weight*lr*target for weight in input_vector]\n",
    "#                         self.weight_vectors[predicted_class] -= [weight*lr for weight in input_vector]\n",
    "                    \n",
    "         \n",
    "    def predict(self,feature_dictionary):\n",
    "        \n",
    "        #To check whether our model can generalise well as it has been trained so it has to learn well to Generalize well\n",
    "        img = cv2.imread(feature_dictionary['path'])\n",
    "        dim = (70,70)\n",
    "        resized_image = cv2.resize(img,dim,cv2.INTER_AREA)\n",
    "        \n",
    "        input_array = []\n",
    "\n",
    "        #Normalising the Image between 0 and 1\n",
    "\n",
    "        input_array = [np.mean(resized_image[i][j]/255) for i in range(resized_image.shape[0])\n",
    "                      for j in range(resized_image.shape[1])]\n",
    "        \n",
    "        \n",
    "        input_array.append(bias)\n",
    "        feature_vector = np.array(input_array)\n",
    "        \n",
    "        \n",
    "        arg_max,predicted_class = 0,self.classes[0]\n",
    "        \n",
    "        #This is the Input for Hidden Layers\n",
    "        input_array_hidden = []\n",
    "\n",
    "        #We Make MultiClass Decision Rule Based on the above conditions\n",
    "        for index in self.hidden:\n",
    "            current_activation = np.dot(feature_vector,self.weight_vectors[index])\n",
    "            current_activation = ReLu(current_activation)\n",
    "                    \n",
    "            input_array_hidden.append(current_activation)\n",
    "                \n",
    "        #We Make MultiClass Decision Rule Based on the above conditions\n",
    "        for index in self.classes:\n",
    "            current_activation = np.dot(input_array_hidden,self.weight_hidd_out[index])\n",
    "            #current_activation = ReLu(current_activation)\n",
    "            if current_activation >= arg_max:\n",
    "                arg_max,predicted_class = current_activation,index\n",
    "                \n",
    "        return predicted_class\n",
    "    \n",
    "    def test_random_data(self):\n",
    "        item = random.choice(self.test_set)\n",
    "        print(\"Actual Class :\" + item[0])\n",
    "        pred_class = self.predict(item[1])\n",
    "        print(\"Predicted Class :\" + pred_class)\n",
    "        \n",
    "        \n",
    "    def calculate_accuracy(self):\n",
    "        \n",
    "        \"\"\"Calculates the accuracy of the classifier by running algorithm against test set and comparing\n",
    "        the output to the actual categorization.\"\"\"\n",
    "        \n",
    "        correct,incorrect = 0,0\n",
    "        random.shuffle(self.feature_data)\n",
    "        self.test_set = self.feature_data[int(len(self.feature_data)*self.ratio):]\n",
    "        for feature_dictionary in self.test_set:\n",
    "            actual_class = feature_dictionary[0]\n",
    "            predicted_class = self.predict(feature_dictionary[1])\n",
    "            \n",
    "            if actual_class == predicted_class:\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect += 1\n",
    "                \n",
    "        print(\"ACCURACY\")\n",
    "        print(\"Accuracy of FF Model is \",(correct * 1.0) / ((correct + incorrect) * 1.0))\n",
    "        \n",
    "    \n",
    "    def select_random_picture(self):\n",
    "        #Reading Individual Image For Classification\n",
    "        \n",
    "        s = glob.glob(os.getcwd() + '\\\\Testing_Sample/*')\n",
    "        \n",
    "        #Rreading the Image Using Open CV\n",
    "        img = cv2.imread(s[0])\n",
    "        dim = (70,70)\n",
    "        resized_image = cv2.resize(img,dim,cv2.INTER_AREA)\n",
    "        \n",
    "        input_array = []\n",
    "\n",
    "        #Normalising the Image between 0 and 1\n",
    "\n",
    "        input_array = [np.mean(resized_image[i][j]/255) for i in range(resized_image.shape[0])\n",
    "                      for j in range(resized_image.shape[1])]\n",
    "        \n",
    "        \n",
    "        input_array.append(bias)\n",
    "        feature_vector = np.array(input_array)\n",
    "        \n",
    "        \n",
    "        arg_max,predicted_class = 0,self.classes[0]\n",
    "                \n",
    "        #We Make MultiClass Decision Rule Based on the above conditions\n",
    "        for index in self.classes:\n",
    "            current_activation = np.dot(feature_vector,self.weight_vectors[index])\n",
    "            if current_activation >= arg_max:\n",
    "                arg_max,predicted_class = current_activation,index\n",
    "                \n",
    "        print(\"The Predicted Class for the Sample Image is\")\n",
    "        print(predicted_class)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The MAin function driver function\n",
    "if __name__ == \"__main__\":\n",
    "    hidden_neuron = [i for i in range(500)]\n",
    "    classifier = MultiClassPerceptron(classes,feature_data,hidden_neuron)\n",
    "    classifier.train()\n",
    "    classifier.calculate_accuracy()\n",
    "    #classifier.select_random_picture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
